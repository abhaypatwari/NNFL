{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q9.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1KmKORp0qDPmMGMG4Rwl7DHrlAeT659OP","authorship_tag":"ABX9TyOjaI8wln/ID9NZxKlULLKA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Qd_WPBGVhPfH"},"source":["import numpy as np\n","from itertools import islice\n","from scipy.stats import multivariate_normal\n","\n","def performance(y, pred):\n","  m = np.zeros((3,3)) # confusion matrix\n","  for p in range(len(pred)):\n","    if pred[p]==1 and y[p]==1:\n","      m[0,0]+=1\n","    if pred[p]==2 and y[p]==2:\n","      m[1,1]+=1\n","    if pred[p]==3 and y[p]==3:\n","      m[2,2]+=1\n","    if pred[p]==1 and y[p]==2:\n","      m[1,0]+=1\n","    if pred[p]==1 and y[p]==3:\n","      m[2,0]+=1\n","    if pred[p]==2 and y[p]==1:\n","      m[0,1]+=1\n","    if pred[p]==2 and y[p]==3:\n","      m[2,1]+=1\n","    if pred[p]==3 and y[p]==1:\n","      m[0,2]+=1\n","    if pred[p]==3 and y[p]==2:\n","      m[1,2]+=1\n","  ind_accuracy = [m[0,0]/np.sum(m[0,:]), m[1,1]/np.sum(m[1,:]), m[2,2]/np.sum(m[2,:])]\n","  accuracy = (m[0,0]+m[1,1]+m[2,2])/np.sum(m)\n","  return ind_accuracy, accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKJRYuaThybC"},"source":["# getting data from file\n","data = []\n","filename = '/content/drive/MyDrive/NNFL Assignments (Aug 2021)/Assignment 1/data_q6_q7.txt'\n","with open(filename) as file:\n","  lines = list(islice(file,None,None,None))\n","  for line in lines:\n","    entries = line.split('\\t')\n","    while '' in entries: entries.remove('')\n","    y = int(entries[-1].replace('\\n',''))\n","    entries.pop()\n","    v = list(map(float,entries))\n","    v.append(y)\n","    data.append(v)\n","\n","data = np.concatenate((np.ones((np.shape(data)[0],1)),np.array(data)), axis=1) #appending ones\n","np.random.shuffle(data) # shuffling data\n","x = np.array(data[:,:np.shape(data)[1]-1])\n","y = np.array(data[:,-1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OURMeq8h38t"},"source":["m = len(y)\n","nf = 5 #number of folds\n","x_subsets = np.array_split(x, nf)\n","y_subsets = np.array_split(y, nf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htRYauYLj3Ra","executionInfo":{"status":"ok","timestamp":1632802708792,"user_tz":-330,"elapsed":375,"user":{"displayName":"Abhay Patwari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gix32BX_ssvRlYOdFlU8riZsgPg3lrL-_h0Lvui=s64","userId":"16538957145694214892"}},"outputId":"1fdd2787-50cd-405d-e328-da1c9bf94bc0"},"source":["# MAP Classifier\n","accuracy_vals = []\n","ind_accuracy1 = []\n","ind_accuracy2 = []\n","ind_accuracy3 = []\n","\n","for fold in range(nf):\n","  # test-train split\n","  x_test = x_subsets[fold]\n","  y_test = y_subsets[fold]\n","\n","  x_train = np.concatenate(np.delete(x_subsets, fold, 0), axis=0)\n","  y_train = np.concatenate(np.delete(y_subsets, fold, 0), axis=0)\n","\n","  m_train = len(y_train)\n","  m_test = len(y_test)\n","\n","  # normalizing input data\n","  pp = np.amax(np.abs(x_train), axis=0)\n","  x_train = x_train/pp\n","  x_test = x_test/pp\n","\n","  # data for likelihood\n","  x_class1_train = x_train[np.where(y_train==1)]\n","  x_class2_train = x_train[np.where(y_train==2)]\n","  x_class3_train = x_train[np.where(y_train==3)]\n","\n","  y_class1_train = y_train[np.where(y_train==1)]\n","  y_class2_train = y_train[np.where(y_train==2)]\n","  y_class3_train = y_train[np.where(y_train==3)]\n","\n","  # likelihood modeled by multivariate gaussian\n","  # pdfXgiven1 = multivariate_normal.pdf(x_test, np.mean(x_class1_train, axis=0), np.cov(x_class1_train, rowvar=False), allow_singular=True)\n","  # pdfXgiven2 = multivariate_normal.pdf(x_test, np.mean(x_class2_train, axis=0), np.cov(x_class1_train, rowvar=False), allow_singular=True)\n","  # pdfXgiven3 = multivariate_normal.pdf(x_test, np.mean(x_class3_train, axis=0), np.cov(x_class3_train, rowvar=False), allow_singular=True)\n","\n","  mean1 = np.mean(x_class1_train, axis=0)\n","  mean2 = np.mean(x_class2_train, axis=0)\n","  mean3 = np.mean(x_class3_train, axis=0)\n","\n","  cov1 =np.cov(x_class1_train, rowvar=False)\n","  cov2 =np.cov(x_class1_train, rowvar=False)\n","  cov2 =np.cov(x_class3_train, rowvar=False)\n","\n","  pdfXgiven1 = np.exp(-1*x_test/mean1)/mean1\n","  pdfXgiven2 = np.exp(-1*x_test/mean2)/mean2\n","  pdfXgiven3 = np.exp(-1*x_test/mean3)/mean3\n","\n","  pofy1 = len(y_class1_train)/m_train\n","  pofy2 = len(y_class2_train)/m_train\n","  pofy3 = len(y_class3_train)/m_train\n","\n","  # not considering evidence as it is constant for all classes\n","\n","  MAP_class1 = pdfXgiven1 * pofy1\n","  MAP_class2 = pdfXgiven2 * pofy2\n","  MAP_class3 = pdfXgiven3 * pofy3\n","\n","  # prediction\n","  pred = []\n","  for i in range(m_test):\n","    MAP_vector = [MAP_class1[i],MAP_class2[i],MAP_class3[i]]\n","    pred.append(np.argmax(MAP_vector)+1)\n","\n","  # performance measures\n","  ind_accuracy, accuracy = performance(y_test, pred)\n","  accuracy_vals.append(accuracy)\n","  ind_accuracy1.append(ind_accuracy[0])\n","  ind_accuracy2.append(ind_accuracy[1])\n","  ind_accuracy3.append(ind_accuracy[2])\n","\n","mean_accuracy = np.mean(accuracy_vals)\n","mean_accuracy1 = np.mean(ind_accuracy1)\n","mean_accuracy2 = np.mean(ind_accuracy2)\n","mean_accuracy3 = np.mean(ind_accuracy3)\n","\n","print(\"mean accuracy of class 1 = {}\".format(mean_accuracy1))\n","print(\"mean accuracy of class 2 = {}\".format(mean_accuracy3))\n","print(\"mean accuracy of class 3 = {}\".format(mean_accuracy3))\n","print(\"mean accuracy of classifier = {}\".format(mean_accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean accuracy of class 1 = 0.9125506072874494\n","mean accuracy of class 2 = 0.971291866028708\n","mean accuracy of class 3 = 0.971291866028708\n","mean accuracy of classifier = 0.9523809523809523\n"]}]}]}