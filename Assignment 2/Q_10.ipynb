{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIRvIDnOLa5G"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint,sample\n",
        "import time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sFxksGQMZqe",
        "outputId": "270f68b0-852f-475f-f04f-13191e7028bd"
      },
      "source": [
        "!git clone https://github.com/guru-narayana/NNFL-assignment"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NNFL-assignment'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 18 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WahTQLHmMcws"
      },
      "source": [
        "## loading the data\n",
        "data_frame = pd.read_excel(\"/content/NNFL-assignment/data5.xlsx\").sample(frac = 1)\n",
        "data = data_frame.to_numpy()\n",
        "\n",
        "## seperating input and output data\n",
        "input_prenorm_data = data[:,0:data.shape[1]-1]\n",
        "op = data[:,data.shape[1]-1]\n",
        "\n",
        "## normalising the data\n",
        "max_data  =  np.amax(input_prenorm_data,axis=0) ## max  values in each column\n",
        "input_data  = input_prenorm_data/max_data\n",
        "output_data = np.zeros((len(op),3))\n",
        "\n",
        "#Auto encoding\n",
        "for i in range(len(op)):\n",
        "  output_data[i,int(op[i]-1)] = 1\n",
        "\n",
        "trim_point_1 = int(input_data.shape[0]*0.7)\n",
        "trim_point_2 = int(input_data.shape[0]*0.8)\n",
        "\n",
        "\n",
        "input_train_data      = input_data[0:trim_point_1,:]\n",
        "input_validation_data = input_data[trim_point_1:trim_point_2,:]\n",
        "input_test_data       = input_data[trim_point_2:input_data.shape[0],:]\n",
        "\n",
        "output_train_data      = output_data[0:trim_point_1]\n",
        "output_validation_data = output_data[trim_point_1:trim_point_2]\n",
        "output_test_data       = output_data[trim_point_2:input_data.shape[0]]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCeBAkfeR2TF",
        "outputId": "29983f99-5899-4d1e-f381-169fa07e3cc6"
      },
      "source": [
        "np.random.rand(input_data.shape[1],3,input_data.shape[1])[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.57115821, 0.84041566, 0.57215039, 0.57931068, 0.95464187,\n",
              "        0.56720198, 0.98460973],\n",
              "       [0.27159091, 0.27164001, 0.82151444, 0.84659854, 0.95998421,\n",
              "        0.20030849, 0.33682338],\n",
              "       [0.31891127, 0.27567102, 0.8909302 , 0.8381022 , 0.90021267,\n",
              "        0.36467728, 0.45344372]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51bFJzm-Nwbl"
      },
      "source": [
        "def perceptron(W,X,B):\n",
        "  P = (X@W.T) + B\n",
        "  return 1/(1 + np.exp(-P))\n",
        "\n",
        "\n",
        "def confiusion_matrix_func(outpt,Y):\n",
        "  pred = np.argmax(outpt, axis=1)+1\n",
        "  output_test_data = np.argmax(Y, axis=1)+1\n",
        "  confusion_matrix = np.zeros((3,3))\n",
        "  count = 0\n",
        "  for x in range(len(pred)):\n",
        "    if(pred[x] == output_test_data[x]):\n",
        "      count+=1\n",
        "    if(pred[x] == 1):\n",
        "      if(output_test_data[x] == 1):\n",
        "        confusion_matrix[0][0]+=1\n",
        "      elif(output_test_data[x] == 2):\n",
        "        confusion_matrix[1][0]+=1\n",
        "      elif(output_test_data[x] == 3):\n",
        "        confusion_matrix[2][0]+=1\n",
        "    elif(pred[x] == 2):\n",
        "      if(output_test_data[x]   == 1):\n",
        "        confusion_matrix[0][1]+=1\n",
        "      elif(output_test_data[x] == 2):\n",
        "        confusion_matrix[1][1]+=1\n",
        "      elif(output_test_data[x] == 3):\n",
        "        confusion_matrix[2][1]+=1\n",
        "    elif(pred[x] == 3):\n",
        "      if(output_test_data[x]   == 1):\n",
        "        confusion_matrix[0][2]+=1\n",
        "      elif(output_test_data[x] == 2):\n",
        "        confusion_matrix[1][2]+=1\n",
        "      elif(output_test_data[x] == 3):\n",
        "        confusion_matrix[2][2]+=1\n",
        "  c1 = confusion_matrix[0][0]/np.sum(confusion_matrix[0][:])\n",
        "  c2 = confusion_matrix[1][1]/np.sum(confusion_matrix[1][:])\n",
        "  c3 = confusion_matrix[2][2]/np.sum(confusion_matrix[2][:])\n",
        "  A  = count/len(pred)\n",
        "  print(\"class_1  Accuracy  : \"+ str( c1))\n",
        "  print(\"class_2  Accuracy  : \"+ str( c2))\n",
        "  print(\"class_3  Accuracy  : \"+ str( c3))\n",
        "  print( \"Overall Accuracy  : \"+ str( A))\n",
        "  print(\"\\n\")\n",
        "\n",
        "def HFDNN(alpha,W1,W2,W3,B1,B2,B3,iterations,input_data,output_data):\n",
        "\n",
        "\n",
        "  fuzzyfication_size = 6\n",
        "  WF = np.random.rand(output_data.shape[1],fuzzyfication_size)\n",
        "  BF = np.random.rand(output_data.shape[1])\n",
        "  mu = np.random.rand(input_data.shape[1],fuzzyfication_size,input_data.shape[1])\n",
        "  sigma = np.random.rand(input_data.shape[1],fuzzyfication_size)\n",
        "  \n",
        "  \n",
        "  \n",
        "  for i in range(iterations):\n",
        "\n",
        "    layer1 = np.random.rand(input_data.shape[0],input_data.shape[1]*fuzzyfication_size)\n",
        "    counter = 0\n",
        "    for i in range(input_data.shape[1]):\n",
        "      for j in range(fuzzyfication_size):\n",
        "        layer1[:,counter] = np.exp(-np.sum((input_data - mu[i][j])**2,axis=1)/(sigma[i,j]**2))\n",
        "        counter+=1\n",
        "\n",
        "    layer2 = np.random.rand(input_data.shape[0],fuzzyfication_size)\n",
        "    for i in range(fuzzyfication_size):\n",
        "      for j in range(input_data.shape[1]):\n",
        "        layer2[:,i] *= layer1[:,fuzzyfication_size*j +i]\n",
        "\n",
        "    H1_output = perceptron(W1,input_data,B1)\n",
        "    H2_output = perceptron(W2,H1_output,B2)\n",
        "    Y_NN      = perceptron(W3,H2_output,B3)\n",
        "\n",
        "    Y = layer2@WF.T + Y_NN\n",
        "    error  = -(Y - output_data)\n",
        "    cost = np.sum(error**2)/(input_data.shape[0]*output_data.shape[1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    deltaF = np.dot(np.dot(error,WF).T,layer2)\n",
        "    deltaF1 = np.dot(np.dot(error,WF),layer2.T)\n",
        "\n",
        "    for i in range(input_data.shape[1]):\n",
        "      for j in range(fuzzyfication_size):\n",
        "        mu[i][j] += 0.001*alpha*np.dot(deltaF1[j],np.sum((input_data - mu[i][j]),axis=1)/(sigma[i,j]**2))\n",
        "\n",
        "\n",
        "    for i in range(input_data.shape[1]):\n",
        "      for j in range(fuzzyfication_size):\n",
        "        sigma[i][j] += 100*alpha*np.sum(np.dot(deltaF1[j],np.sum((input_data - mu[i][j])**2,axis=1))/(sigma[i,j]**2))\n",
        "    WF += alpha*np.dot(error.T,layer2) \n",
        "\n",
        "\n",
        "\n",
        "    delta3 = error*Y*(1-Y)\n",
        "    delta2 = np.dot(delta3,W3)*H2_output*(1-H2_output)\n",
        "    delta1 = np.dot(delta2,W2)*H1_output*(1-H1_output)\n",
        "    W3 += alpha*np.dot(delta3.T,H2_output)\n",
        "    W2 += alpha*np.dot(delta2.T,H1_output)\n",
        "    W1 += alpha*np.dot(delta1.T,input_data)\n",
        "    B3 += alpha*np.sum(delta3,axis=0)\n",
        "    B2 += alpha*np.sum(delta2,axis=0)\n",
        "    B1 += alpha*np.sum(delta1,axis=0)\n",
        "\n",
        "  layer1 = np.random.rand(input_test_data.shape[0],input_test_data.shape[1]*fuzzyfication_size)\n",
        "  counter = 0\n",
        "  for i in range(input_test_data.shape[1]):\n",
        "    for j in range(fuzzyfication_size):\n",
        "      layer1[:,counter] = np.exp(-np.sum((input_test_data - mu[i][j])**2,axis=1)/(sigma[i,j]**2))\n",
        "      counter+=1\n",
        "\n",
        "  layer2 = np.random.rand(input_test_data.shape[0],fuzzyfication_size)\n",
        "  for i in range(fuzzyfication_size):\n",
        "    for j in range(input_test_data.shape[1]):\n",
        "      layer2[:,i] *= layer1[:,fuzzyfication_size*j +i]\n",
        "\n",
        "  H1_output = perceptron(W1,input_test_data,B1)\n",
        "  H2_output = perceptron(W2,H1_output,B2)\n",
        "  Y_NN        = perceptron(W3,H2_output,B3)\n",
        "\n",
        "  Y = layer2@WF.T + Y_NN\n",
        "  confiusion_matrix_func(Y,output_test_data)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsWm0fPB704Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00178a0-f29f-4d34-8340-08c3acf36ef7"
      },
      "source": [
        "hidden_layer_size1 = 10\n",
        "hidden_layer_size2 = 5\n",
        "W1 = np.random.rand(hidden_layer_size1,input_data.shape[1])\n",
        "W2 = np.random.rand(hidden_layer_size2,hidden_layer_size1)\n",
        "W3 = np.random.rand(output_data.shape[1],hidden_layer_size2)\n",
        "B1 = np.random.rand(hidden_layer_size1)\n",
        "B2 = np.random.rand(hidden_layer_size2)\n",
        "B3 = np.random.rand(output_data.shape[1])\n",
        "\n",
        "HFDNN(0.1,W1,W2,W3,B1,B2,B3,10000,input_train_data,output_train_data)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_1  Accuracy  : 0.9411764705882353\n",
            "class_2  Accuracy  : 1.0\n",
            "class_3  Accuracy  : 0.8\n",
            "Overall Accuracy  : 0.9285714285714286\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}