{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Q5.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ak7C9aBXs04L-4v7BooASONLy9KOUOpr","authorship_tag":"ABX9TyOv2a7yqaSJDrHWRQ9I+Kf1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Hn1eOYjxRmwQ"},"source":["import numpy as np\n","import pandas as pd\n","\n","def sigmoid(x,w):\n","  arg = np.dot(x,w.T)\n","  return 1/(1 + np.exp(-arg))\n","\n","def pred(y):\n","  return (np.argmax(y, axis=1) + 1)\n","\n","def holdout(data, trainp, validp):\n","  m = np.shape(data)[0]\n","\n","  train = data[0 : int(np.floor(m*trainp/100))]\n","  valid = data[int(np.floor(m*trainp/100)) : int(np.floor(m*trainp/100))+int(np.floor(m*validp/100))]\n","  test = data[int(np.floor(m*trainp/100))+int(np.floor(m*validp/100)) : None]\n","\n","  y_train = train[:,-1] #shape = (rows,)\n","  x_train = np.delete(train, -1, axis=1)\n","\n","  y_valid = valid[:,-1] #shape = (rows,)\n","  x_valid = np.delete(valid, -1, axis=1)\n","\n","  y_test = test[:,-1] #shape = (rows,)\n","  x_test = np.delete(test, -1, axis=1)\n","\n","  return x_train,x_valid,x_test,y_train,y_valid,y_test\n","\n","def appendones(x):\n","  m = np.shape(x)[0]\n","  return np.concatenate((np.ones((m,1)),x), axis=1)\n","\n","def one_hot_encode(y):\n","  len = np.size(y)\n","  encoded_y = np.zeros((len,3))\n","  for i in range(len):\n","    if y[i] == 1:\n","      encoded_y[i,0] = 1\n","    if y[i] == 2:\n","      encoded_y[i,1] = 1\n","    if y[i] == 3:\n","      encoded_y[i,2] = 1\n","  return encoded_y\n","\n","def performance(y, pred):\n","  m = np.zeros((3,3)) # confusion matrix\n","  for p in range(len(pred)):\n","    if pred[p]==1 and y[p]==1:\n","      m[0,0]+=1\n","    if pred[p]==2 and y[p]==2:\n","      m[1,1]+=1\n","    if pred[p]==3 and y[p]==3:\n","      m[2,2]+=1\n","    if pred[p]==1 and y[p]==2:\n","      m[1,0]+=1\n","    if pred[p]==1 and y[p]==3:\n","      m[2,0]+=1\n","    if pred[p]==2 and y[p]==1:\n","      m[0,1]+=1\n","    if pred[p]==2 and y[p]==3:\n","      m[2,1]+=1\n","    if pred[p]==3 and y[p]==1:\n","      m[0,2]+=1\n","    if pred[p]==3 and y[p]==2:\n","      m[1,2]+=1\n","  ind_accuracy = [m[0,0]/np.sum(m[0,:]), m[1,1]/np.sum(m[1,:]), m[2,2]/np.sum(m[2,:])]\n","  accuracy = (m[0,0]+m[1,1]+m[2,2])/np.sum(m)\n","  return ind_accuracy, accuracy\n","\n","def auto_encoder(x,alpha,lamda,iters):\n","  W1 = np.random.rand(x.shape[1]-1,x.shape[1])\n","  W2 = np.random.rand(x.shape[1],x.shape[1]-1)\n","  \n","  for i in range(iters):\n","    H1 = sigmoid(x,W1)\n","    H2 = sigmoid(H1,W2)\n","    error  = (H2-x)\n","    delta2 = error*H2*(1-H2)\n","    delta1 = np.dot(delta2,W2)*H1*(1-H1)\n","    W2 -= alpha*(np.dot(delta2.T,H1) + lamda*W2)\n","    W1 -= alpha*(np.dot(delta1.T,x) + lamda*W1)\n","    \n","  return W1,sigmoid(x,W1)\n","\n","def stacked_auto_encoder(alpha,lamda,iters):\n","  # pretraining \n","  W1,H1 = auto_encoder(x_train,0.06,0.00001,10000)\n","  W2,H2 = auto_encoder(H1,0.06,0.00001,10000)\n","  W3,H3 = auto_encoder(H2,0.03,0.00001,10000)\n","\n","  # fine tuning \n","  W4 = np.random.rand(H3.shape[1]-2,H3.shape[1])\n","  alpha1 = 100*alpha\n","  for i in range(iters):\n","    H4 = sigmoid(H3,W4)\n","    error  = (H4-y_train_coded)\n","    delta4 = error*H4*(1-H4)\n","    delta3 = np.dot(delta4,W4)*H3*(1-H3)\n","    delta2 = np.dot(delta3,W3)*H2*(1-H2)\n","    delta1 = np.dot(delta2,W2)*H1*(1-H1)\n","    W4 -= alpha1*(np.dot(delta4.T,H3) + lamda*W4)\n","    W3 -= alpha*(np.dot(delta3.T,H2) + lamda*W3)\n","    W2 -= alpha*(np.dot(delta2.T,H1) + lamda*W2)\n","    W1 -= alpha*(np.dot(delta1.T,x_train) + lamda*W1)\n","    \n","  y = sigmoid(sigmoid(sigmoid(sigmoid(x_test,W1),W2),W3),W4)\n","  return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSP8C5afSzbW"},"source":["data = pd.read_excel('/content/drive/MyDrive/NNFL Assignments (Aug 2021)/Assignment 2/data5.xlsx')\n","cols = np.array(data.columns, ndmin=2)\n","data = data.to_numpy()\n","data = np.concatenate((cols,data), axis=0)\n","\n","# shuffle & train-test-valid-split\n","np.random.seed(0)\n","np.random.shuffle(data)\n","x_train, x_valid, x_test, y_train, y_valid, y_test = holdout(data, 70, 10)\n","\n","# normalizing input data\n","mu = np.mean(x_train, axis=0)\n","std = np.std(x_train, axis=0)\n","\n","x_train = (x_train-mu)/std\n","x_valid = (x_valid-mu)/std\n","x_test = (x_test-mu)/std\n","\n","# one-hot-encoding output data\n","y_train_coded = one_hot_encode(y_train)\n","y_valid_coded = one_hot_encode(y_valid)\n","y_test_coded = one_hot_encode(y_test)\n","\n","# appending ones (input for bias)\n","x_train = appendones(x_train)\n","x_valid = appendones(x_valid)\n","x_test = appendones(x_test)\n","\n","m_train = np.size(y_train)\n","m_valid = np.size(y_valid)\n","m_test = np.size(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceY_78OUYZtK","executionInfo":{"status":"ok","timestamp":1637917878317,"user_tz":-330,"elapsed":3050,"user":{"displayName":"Abhay Patwari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gix32BX_ssvRlYOdFlU8riZsgPg3lrL-_h0Lvui=s64","userId":"16538957145694214892"}},"outputId":"75cab486-5b74-433f-d5d6-bf281b455409"},"source":["y = stacked_auto_encoder(0.0001,0.01,1000)\n","\n","# performance measures\n","ind_accuracy, accuracy = performance(y_test, pred(y))\n","print(\"accuracy of class 1 = {}\".format(ind_accuracy[0]))\n","print(\"accuracy of class 2 = {}\".format(ind_accuracy[1]))\n","print(\"accuracy of class 3 = {}\".format(ind_accuracy[2]))\n","print(\"overall accuracy of classifier = {}\".format(accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy of class 1 = 0.9\n","accuracy of class 2 = 1.0\n","accuracy of class 3 = 0.8947368421052632\n","overall accuracy of classifier = 0.9285714285714286\n"]}]}]}